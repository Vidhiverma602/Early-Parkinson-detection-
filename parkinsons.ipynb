{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the HOG features from images\n",
    "from skimage import feature\n",
    "def quantify_image(image):\n",
    "    features = feature.hog(image, orientations=9,\n",
    "                           pixels_per_cell=(10, 10), cells_per_block=(2, 2),\n",
    "                           transform_sqrt=True, block_norm=\"L1\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the dataset, i.e., converting the images into features and labels\n",
    "import cv2\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import os\n",
    "def process_dataset(path):\n",
    "  imagePaths = list(paths.list_images(path))  #path = path of the dataset \n",
    "  data = []\n",
    "  labels = []\n",
    "  for imagePath in imagePaths: #iterating through each image eg. imagePath = /content/spiral/testing/parkinson/V07PE01.png\n",
    "    # print(imagePath)\n",
    "    label = imagePath.split(os.path.sep)[-2] #getting label of each image, i.e., parkinson or healthy\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (200, 200))\n",
    "    image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    features = quantify_image(image)  #extracting features of image\n",
    "    data.append(features) \n",
    "    labels.append(label)\n",
    "  return (np.array(data), np.array(labels)) #returning the feature array and the corresponding label array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spiliting the dataset into testing and training and returning the feature and labels array for testing and training\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import StandardScaler     \n",
    "import os\n",
    "def select_and_split_dataset(data_set):\n",
    "  path = './drawings/'+data_set   #defining the path for this google colab, must be changed accordingly for different platforms\n",
    "  trainingPath = os.path.sep.join([path, \"training\"])\n",
    "  testingPath = os.path.sep.join([path, \"testing\"])\n",
    "  # load the data\n",
    "  trainX, trainY = process_dataset(trainingPath)\n",
    "  testX, testY = process_dataset(testingPath)\n",
    "  # st_x= StandardScaler()  \n",
    "  # trainX= st_x.fit_transform(trainX)    \n",
    "  # testX= st_x.transform(testX)  \n",
    "  le = LabelEncoder() # LabelEncoder encodes the label arrays into binary (Parkinsons = 1, Healthy = 0)\n",
    "  trainY = le.fit_transform(trainY)\n",
    "  testY = le.transform(testY)\n",
    "  return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model with the test data\n",
    "#returns the predicted values and prints the confusion matrix and accuracy score for different models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "def test(model, testX, testY):\n",
    "  predY = model.predict(testX)\n",
    "  cm = confusion_matrix(testY, predY)\n",
    "  print('confusion matrix = ',cm)\n",
    "  accuracy = accuracy_score(testY, predY)\n",
    "  print('accuracy score = ',accuracy)\n",
    "  return predY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to pre-define different models to shorten the code\n",
    "models={\n",
    "    'RF': RandomForestClassifier(random_state=10),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'SVM': LinearSVC(C=1, random_state=1),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to store the predictions of spiral model\n",
    "prediction_spiral = {\n",
    "    'RF' : [],\n",
    "    'XGBoost' : [],\n",
    "    'SVM' : [],\n",
    "    'KNN' : [],\n",
    "    'DecisionTree' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to store the predictions of wave model\n",
    "prediction_wave = {\n",
    "    'RF' : [],\n",
    "    'XGBoost' : [],\n",
    "    'SVM' : [],\n",
    "    'KNN' : [],\n",
    "    'DecisionTree' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting and splitting both the datasets\n",
    "spiral = 'spiral'\n",
    "strainX, strainY, stestX, stestY = select_and_split_dataset(spiral)\n",
    "wave = 'wave'\n",
    "wtrainX, wtrainY, wtestX, wtestY = select_and_split_dataset(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train the models\n",
    "def train_model(model, trainX, trainY):\n",
    "  model.fit(trainX, trainY)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating through the models dictionary and training each model for spiral dataset\n",
    "for model in models:\n",
    "  model_spiral = train_model(models[model], strainX, strainY)\n",
    "  with open('./spiral_models/spiral_'+model+'_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_spiral, f)    #saving the model as a pickle file with name 'spiral_{model_name}_model.pickel'\n",
    "#iterating through the models dictionary and training each model for wave dataset\n",
    "for model in models:\n",
    "  model_wave = train_model(models[model], wtrainX, wtrainY)\n",
    "  with open('./wave_models/wave_'+model+'_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_spiral, f)     #saving the model as a pickle file  with name 'wave_{model_name}_model.pickel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model =  RF\n",
      "confusion matrix =  [[13  2]\n",
      " [ 4 11]]\n",
      "accuracy score =  0.8\n",
      "-----------------------\n",
      "model =  XGBoost\n",
      "confusion matrix =  [[11  4]\n",
      " [ 4 11]]\n",
      "accuracy score =  0.7333333333333333\n",
      "-----------------------\n",
      "model =  SVM\n",
      "confusion matrix =  [[11  4]\n",
      " [ 4 11]]\n",
      "accuracy score =  0.7333333333333333\n",
      "-----------------------\n",
      "model =  KNN\n",
      "confusion matrix =  [[11  4]\n",
      " [ 2 13]]\n",
      "accuracy score =  0.8\n",
      "-----------------------\n",
      "model =  DecisionTree\n",
      "confusion matrix =  [[11  4]\n",
      " [ 4 11]]\n",
      "accuracy score =  0.7333333333333333\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "#printing the confusion matrix and accuracy of different models for spiral dataset\n",
    "#and storing the predicted array in prediction_spiral dictionary\n",
    "for model in models:\n",
    "  print('model = ',model)\n",
    "  with open('./spiral_models/spiral_'+model+'_model.pkl', 'rb') as f:\n",
    "    prediction_spiral[model] = test(pickle.load(f), stestX, stestY)\n",
    "  print('-----------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model =  RF\n",
      "confusion matrix =  [[ 1 14]\n",
      " [ 1 14]]\n",
      "accuracy score =  0.5\n",
      "-----------------------\n",
      "model =  XGBoost\n",
      "confusion matrix =  [[ 1 14]\n",
      " [ 1 14]]\n",
      "accuracy score =  0.5\n",
      "-----------------------\n",
      "model =  SVM\n",
      "confusion matrix =  [[ 1 14]\n",
      " [ 1 14]]\n",
      "accuracy score =  0.5\n",
      "-----------------------\n",
      "model =  KNN\n",
      "confusion matrix =  [[ 1 14]\n",
      " [ 1 14]]\n",
      "accuracy score =  0.5\n",
      "-----------------------\n",
      "model =  DecisionTree\n",
      "confusion matrix =  [[9 6]\n",
      " [6 9]]\n",
      "accuracy score =  0.6\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "#printing the confusion matrix and accuracy of different models for wave dataset\n",
    "#and storing the predicted array in prediction_wave dictionary \n",
    "for model in models:\n",
    "  print('model = ',model)\n",
    "  with open('./wave_models/wave_'+model+'_model.pkl', 'rb') as f:\n",
    "    prediction_wave[model] = test(pickle.load(f), wtestX, wtestY)\n",
    "  print('-----------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#displaying the output predicted by the random forest model on the test images\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpl_toolkits\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maxes_grid1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageGrid\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdisplay_output_img\u001b[39m(dataset, preds, testY):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "#displaying the output predicted by the random forest model on the test images\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "def display_output_img(dataset, preds, testY):\n",
    "    path = './drawings/'+dataset+'/testing'\n",
    "    imagePaths = list(paths.list_images(path))  #getting the images in the same way as process_dataset method\n",
    "    output = [] #array to store the images with text on them\n",
    "    for i, images in enumerate(imagePaths): #enumerate to simplify comparing the predicted output with the actual output\n",
    "        image = cv2.imread(images)\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "        color = (0, 255, 0) if preds[i] == testY[i] else (255, 0, 0)  #red for false output, green for true\n",
    "        stroke = 2\n",
    "        text = 'Parkinsons' if preds[i] else 'Healthy'  #preds array is in form [1, 1, 0, 0, 0, .....] where ) = healthy, 1 = parkinsons\n",
    "        # print(image)\n",
    "        cv2.putText(image, text, (20, 20), font, 1, color, stroke, cv2.LINE_AA)\n",
    "        output.append(image)  #appending the images with text to output array\n",
    "    for i in range(len(output)):\n",
    "        plt.subplot(6, 5, i+1)  #6, 5 is used since there are 30 test images, needs to be changed accordingly\n",
    "        plt.imshow(output[i])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_output_img('spiral', prediction_spiral['RF'], stestY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_output_img('wave', prediction_wave['RF'], wtestY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
